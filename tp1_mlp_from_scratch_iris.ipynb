{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31286,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkha03/Introduction_to_Machine_Learning/blob/main/tp1_mlp_from_scratch_iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "2a404bad",
      "cell_type": "markdown",
      "source": [
        "# TP1 — Réseau de neurones *from scratch* sur Iris (NumPy)\n",
        "\n",
        "**Module : Deep Learning & Application — INSA CVL**  \n",
        "**Durée : 2 × 1h20**\n",
        "\n",
        "---\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "Vous allez implémenter un petit réseau de neurones multicouches (*MLP*) **sans framework de deep learning** (uniquement NumPy) :\n",
        "\n",
        "- propagation avant (*forward*)\n",
        "- rétropropagation (*backpropagation*)\n",
        "- entraînement par descente de gradient\n",
        "- évaluation (accuracy, matrice de confusion)\n",
        "\n",
        "Jeu de données : **Iris** (150 exemples, 4 features, 3 classes)\n",
        "\n",
        "Architecture cible :\n",
        "\n",
        "\\[\n",
        "4 → 8 → 3\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## Règles\n",
        "\n",
        "**Autorisé** : `numpy`, `matplotlib`, `sklearn` (chargement données + split + métriques)  \n",
        "**Interdit** : `pytorch`, `tensorflow`, `keras`\n",
        "\n",
        "---\n",
        "\n",
        "## À rendre\n",
        "\n",
        "- le notebook complété (toutes les cellules exécutables)\n",
        "- figures demandées (loss, accuracy, confusion matrix)\n",
        "- réponses aux questions (en Markdown)"
      ],
      "metadata": {
        "id": "2a404bad"
      }
    },
    {
      "id": "83e65b23",
      "cell_type": "markdown",
      "source": [
        "## 0) Imports"
      ],
      "metadata": {
        "id": "83e65b23"
      }
    },
    {
      "id": "f207f919",
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:08.597492Z",
          "iopub.execute_input": "2026-02-25T12:11:08.597756Z",
          "iopub.status.idle": "2026-02-25T12:11:12.252315Z",
          "shell.execute_reply.started": "2026-02-25T12:11:08.597728Z",
          "shell.execute_reply": "2026-02-25T12:11:12.251187Z"
        },
        "id": "f207f919"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d3455b25",
      "cell_type": "markdown",
      "source": [
        "### Si une des librairies n'éxiste pas, exécutez la commande correspondante à la librairie manquante :\n",
        "!pip install numpy\n",
        "\n",
        "!pip install matplotlib\n",
        "\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "d3455b25"
      }
    },
    {
      "id": "64d65319",
      "cell_type": "markdown",
      "source": [
        "## 1) Chargement et préparation des données\n",
        "\n",
        "### Présentation du dataset Iris\n",
        "\n",
        "Le dataset **Iris** est un jeu de données classique en apprentissage automatique, introduit par Ronald Fisher en 1936.\n",
        "\n",
        "Il contient :\n",
        "- **150 échantillons** (fleurs)\n",
        "- **3 classes** :\n",
        "  - 0 → *setosa*\n",
        "  - 1 → *versicolor*\n",
        "  - 2 → *virginica*\n",
        "- **4 caractéristiques numériques (features)** pour chaque fleur :\n",
        "  1. longueur du sépale (sepal length)\n",
        "  2. largeur du sépale (sepal width)\n",
        "  3. longueur du pétale (petal length)\n",
        "  4. largeur du pétale (petal width)\n",
        "\n",
        "Chaque exemple est donc un vecteur :\n",
        "\n",
        "X = [x₁, x₂, x₃, x₄]\n",
        "\n",
        "Notre objectif est de prédire la **classe** (0, 1 ou 2) à partir de ces 4 mesures.\n",
        "\n",
        "---\n",
        "\n",
        "### Travail demandé\n",
        "1. Charger Iris avec `load_iris()`\n",
        "2. Récupérer `X` matrice de features (150 × 4) et `y` vecteur des labels (150,)\n",
        "3. Normaliser les features (par exemple standardisation ou min-max)\n",
        "    - **Min-Max scaling** :\n",
        "     X = (X - min) / (max - min)\n",
        "    - ou **Standardisation** :\n",
        "     X = (X - moyenne) / écart-type\n",
        "4. Avec `train_test_split`, séparer les données en :\n",
        "   - 80% entraînement\n",
        "   - 20% test\n",
        "   \n",
        "    Utiliser `stratify=y` pour garder la même proportion de classes dans train et test.\n",
        "5. Convertir `y_train` et `y_test` en **one-hot** : `Y_train`, `Y_test` de forme `(N, 3)`\n",
        "\n",
        "> **Indices :**\n",
        "- Iris a 3 classes (`0,1,2`)\n",
        "- One-hot : si `y=2`, alors `[0,0,1]`, si `y=1`, alors `[0,1,0]`\n",
        "\n",
        "---\n",
        "\n",
        "### Vérifications attendues\n",
        "\n",
        "Après préparation :\n",
        "\n",
        "- `X_train.shape == (N_train, 4)`\n",
        "- `Y_train.shape == (N_train, 3)`\n",
        "- `X_test.shape  == (N_test, 4)`\n",
        "- `Y_test.shape  == (N_test, 3)`\n",
        "\n",
        "Avec environ :\n",
        "- N_train ≈ 120\n",
        "- N_test ≈ 30\n"
      ],
      "metadata": {
        "id": "64d65319"
      }
    },
    {
      "id": "a2e00a3c",
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO 1: charger Iris\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "\n",
        "# TODO 2: normalisation (choisissez une méthode simple)\n",
        "# Exemple min-max :\n",
        "# X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-12)\n",
        "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-12)\n",
        "\n",
        "# TODO 3: split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "\n",
        "def one_hot(y, num_classes):\n",
        "    # TODO: retourner une matrice (len(y), num_classes)\n",
        "    Y = np.zeros((len(y), num_classes))\n",
        "    for i in range(len(y)):\n",
        "        Y[i][y[i]] = 1\n",
        "    return Y\n",
        "\n",
        "Y_train = one_hot(y_train, 3)\n",
        "Y_test  = one_hot(y_test, 3)\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"Y_train:\", Y_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"Y_test :\", Y_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.254478Z",
          "iopub.execute_input": "2026-02-25T12:11:12.254988Z",
          "iopub.status.idle": "2026-02-25T12:11:12.283052Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.254955Z",
          "shell.execute_reply": "2026-02-25T12:11:12.282034Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e00a3c",
        "outputId": "234a74da-21d9-4744-ab59-972c1bb9217a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (120, 4) Y_train: (120, 3)\n",
            "X_test : (30, 4) Y_test : (30, 3)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "5f5669dd-2924-45eb-ae41-252b15d46f77",
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.284683Z",
          "iopub.execute_input": "2026-02-25T12:11:12.285141Z",
          "iopub.status.idle": "2026-02-25T12:11:12.292346Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.285102Z",
          "shell.execute_reply": "2026-02-25T12:11:12.291327Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5669dd-2924-45eb-ae41-252b15d46f77",
        "outputId": "2f2a3bda-6375-4a3d-a818-49ef94488be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 1 0 2 2 1 2 2 1 0 1 2 2 0 1 1 0 2 0 0 2 2 1 1 0 2 2 1 1 0 2 2 1 2\n",
            " 1 2 1 1 1 0 0 1 1 2 2 1 0 2 2 0 0 1 1 0 0 1 2 0 0 1 1 2 1 2 0 0 2 1 1 0 0\n",
            " 2 1 2 0 1 2 2 1 2 0 1 0 0 2 2 1 2 0 0 0 0 0 1 1 1 2 0 2 0 2 0 1 1 1 1 0 2\n",
            " 2 0 1 1 2 0 2 2 2]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "999a1436",
      "cell_type": "markdown",
      "source": [
        "## 2) Implémentation d'une couche linéaire\n",
        "\n",
        "Une couche linéaire (fully-connected) lors du **Forward pass** :\n",
        "\n",
        "$$\n",
        "Z = XW + b\n",
        "$$\n",
        "\n",
        "où :\n",
        "- `X` (matrice d'entrée) : $(N, \\text{in\\_dim})$\n",
        "- `W` (matrice de poids) : $(\\text{in\\_dim}, \\text{out\\_dim})$\n",
        "- `b` (vecteur de biais) : $(1, \\text{out\\_dim})$\n",
        "- `Z` (sortie de la couche) : $(N, \\text{out\\_dim})$\n",
        "\n",
        "---\n",
        "\n",
        "### Backward pass\n",
        "\n",
        "On suppose que l’on reçoit le gradient :\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial Z}\n",
        "\\quad \\text{de dimension } (N, \\text{out\\_dim})\n",
        "$$\n",
        "\n",
        "On calcule alors :\n",
        "\n",
        "#### Gradient par rapport aux poids\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = X^\\top \\frac{\\partial L}{\\partial Z}\n",
        "$$\n",
        "\n",
        "Dimensions :\n",
        "- $X^\\top$ : $(\\text{in\\_dim}, N)$\n",
        "- $\\frac{\\partial L}{\\partial Z}$ : $(N, \\text{out\\_dim})$\n",
        "- Résultat : $(\\text{in\\_dim}, \\text{out\\_dim})$\n",
        "\n",
        "---\n",
        "\n",
        "#### Gradient par rapport au biais\n",
        "\n",
        "On somme sur le batch :\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} =\n",
        "\\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Z_i}\n",
        "$$\n",
        "\n",
        "Ce qui revient en pratique à :\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b}\n",
        "=\n",
        "\\sum_{i=1}^{N}\n",
        "\\frac{\\partial L}{\\partial Z}\n",
        "\\quad \\text{(axis=0)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Gradient par rapport à l'entrée\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial X}\n",
        "=\n",
        "\\frac{\\partial L}{\\partial Z} W^\\top\n",
        "$$\n",
        "\n",
        "Dimensions :\n",
        "- $(N, \\text{out\\_dim}) \\times (\\text{out\\_dim}, \\text{in\\_dim})$\n",
        "- Résultat : $(N, \\text{in\\_dim})$\n",
        "\n",
        "---\n",
        "\n",
        "### Mise à jour des paramètres\n",
        "\n",
        "Avec un learning rate $\\alpha$ :\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\alpha \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Travail demandé\n",
        "Compléter `forward` et `backward`.\n",
        "\n"
      ],
      "metadata": {
        "id": "999a1436"
      }
    },
    {
      "id": "e46f723a",
      "cell_type": "code",
      "source": [
        "\n",
        "class Linear:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        # initialisation simple (petite variance)\n",
        "        self.W = 0.01 * np.random.randn(input_dim, output_dim)\n",
        "        self.b = np.zeros((1, output_dim))\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: stocker X pour backward\n",
        "        self.X = X\n",
        "        Z = X.dot(self.W) + self.b\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ, learning_rate):\n",
        "        # dZ : (N, out_dim)\n",
        "        # TODO: calculer dW, db, dX\n",
        "        #N = dZ.shape[0]\n",
        "        dW = np.transpose(self.X).dot(dZ)\n",
        "        db = np.sum(dZ, axis=0)\n",
        "        dX = dZ.dot(np.transpose(self.W))\n",
        "\n",
        "        # TODO: mise à jour SGD\n",
        "        self.W = self.W - learning_rate*dW\n",
        "        self.b = self.b - learning_rate*db\n",
        "\n",
        "        return dX"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.293900Z",
          "iopub.execute_input": "2026-02-25T12:11:12.294379Z",
          "iopub.status.idle": "2026-02-25T12:11:12.312084Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.294349Z",
          "shell.execute_reply": "2026-02-25T12:11:12.310720Z"
        },
        "id": "e46f723a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9b21479e",
      "cell_type": "markdown",
      "source": [
        "## 3) Activation ReLU\n",
        "\n",
        "$$\n",
        "\\mathrm{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "### Travail demandé\n",
        "Compléter `forward` et `backward`.\n",
        "\n",
        "> Indice : en backward, le gradient passe uniquement là où l'entrée était $> 0$.\n",
        "\n",
        "**Rappel utile pour le backward :**\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}\\mathrm{ReLU}(x) =\n",
        "\\begin{cases}\n",
        "1 & \\text{si } x > 0 \\\\\n",
        "0 & \\text{sinon}\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "9b21479e"
      }
    },
    {
      "id": "63318fa8",
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.input_for_backward = None # Store the input X for proper backward computation\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_for_backward = X # Store the input X for backward pass\n",
        "        out = np.maximum(0, X)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dA, learning_rate=None):\n",
        "        # The gradient passes only where the input X was > 0\n",
        "        return dA * (self.input_for_backward > 0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.313336Z",
          "iopub.execute_input": "2026-02-25T12:11:12.313630Z",
          "iopub.status.idle": "2026-02-25T12:11:12.334120Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.313605Z",
          "shell.execute_reply": "2026-02-25T12:11:12.332939Z"
        },
        "id": "63318fa8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "971aecc7",
      "cell_type": "markdown",
      "source": [
        "## 4) Softmax (version stable)\n",
        "\n",
        "Pour un batch `X` de forme $(N, C)$ :\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$\n",
        "\n",
        "### Version vectorisée (batch)\n",
        "\n",
        "Pour une ligne $x \\in \\mathbb{R}^C$ :\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax}(x)_i =\n",
        "\\frac{e^{x_i}}{\\sum_{j=1}^{C} e^{x_j}}\n",
        "$$\n",
        "\n",
        "### Travail demandé\n",
        "Compléter `forward`.\n",
        "\n",
        "> Indice stabilité : soustraire $\\max(X, \\text{axis}=1, \\text{keepdims}=True)$ avant l'exponentielle.\n",
        "\n",
        "### Forme numériquement stable\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax}(x)_i =\n",
        "\\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{C} e^{x_j - \\max(x)}}\n",
        "$$"
      ],
      "metadata": {
        "id": "971aecc7"
      }
    },
    {
      "id": "de90ed3f",
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: softmax stable\n",
        "        X_shift = X - np.max(X, axis=1, keepdims=True)\n",
        "        exp_X = np.exp(X_shift)\n",
        "        self.out = exp_X / np.sum(exp_X, axis=1, keepdims=True) # Corrected summation axis\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dY, learning_rate=None):\n",
        "        # Bonus: on n'en a pas besoin si on utilise directement le gradient CE+Softmax.\n",
        "        return dY"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.335536Z",
          "iopub.execute_input": "2026-02-25T12:11:12.335964Z",
          "iopub.status.idle": "2026-02-25T12:11:12.351347Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.335879Z",
          "shell.execute_reply": "2026-02-25T12:11:12.350392Z"
        },
        "id": "de90ed3f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "641257ac",
      "cell_type": "markdown",
      "source": [
        "## 5) Cross-Entropy (multi-classes) + gradient\n",
        "\n",
        "Loss cross-entropy avec labels one-hot :\n",
        "\n",
        "$$\n",
        "L = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C} y_{ic}\\,\\log(\\hat{y}_{ic})\n",
        "$$\n",
        "\n",
        "### Travail demandé\n",
        "1. Compléter `cross_entropy_loss`\n",
        "\n",
        "> Important : si `y_pred` vient d'un softmax, le gradient simplifié par rapport aux **logits** (avant softmax) est :\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial Z} = \\frac{\\hat{Y} - Y}{N}\n",
        "$$"
      ],
      "metadata": {
        "id": "641257ac"
      }
    },
    {
      "id": "9d5de825",
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_pred, y_true, eps=1e-12):\n",
        "    '''\n",
        "    y_pred: (N, C) probabilités (softmax)\n",
        "    y_true: (N, C) one-hot\n",
        "    '''\n",
        "    # TODO: calculer la loss moyenne\n",
        "    y_pred_clipped = np.where(y_pred < eps, eps, y_pred)\n",
        "    loss = -1/len(y_true) * sum(\n",
        "            sum(y1_j * np.log(y2_j) for y1_j, y2_j in zip(y1, y2))\n",
        "            for y1, y2 in zip(y_true, y_pred_clipped)\n",
        "            )\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.354456Z",
          "iopub.execute_input": "2026-02-25T12:11:12.354914Z",
          "iopub.status.idle": "2026-02-25T12:11:12.383132Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.354862Z",
          "shell.execute_reply": "2026-02-25T12:11:12.381918Z"
        },
        "id": "9d5de825"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ce87af1d",
      "cell_type": "markdown",
      "source": [
        "## 6) Construction du réseau\n",
        "\n",
        "Architecture :\n",
        "\n",
        "- $\\mathrm{Linear}(4 \\rightarrow 8)$\n",
        "- $\\mathrm{ReLU}$\n",
        "- $\\mathrm{Linear}(8 \\rightarrow 3)$\n",
        "\n",
        "### Forme compacte\n",
        "\n",
        "$$\n",
        "X \\in \\mathbb{R}^{N \\times 4}\n",
        "\\;\\xrightarrow{\\;\\mathrm{Linear}\\;}\n",
        "\\mathbb{R}^{N \\times 8}\n",
        "\\;\\xrightarrow{\\;\\mathrm{ReLU}\\;}\n",
        "\\mathbb{R}^{N \\times 8}\n",
        "\\;\\xrightarrow{\\;\\mathrm{Linear}\\;}\n",
        "\\mathbb{R}^{N \\times 3}\n",
        "\\;\\xrightarrow{\\;\\mathrm{Softmax}\\;}\n",
        "\\hat{Y} \\in \\mathbb{R}^{N \\times 3}\n",
        "$$\n",
        "\n",
        "### Équations\n",
        "\n",
        "$$\n",
        "Z_1 = XW_1 + b_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_1 = \\mathrm{ReLU}(Z_1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Z_2 = A_1 W_2 + b_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{Y} = \\mathrm{softmax}(Z_2)\n",
        "$$"
      ],
      "metadata": {
        "id": "ce87af1d"
      }
    },
    {
      "id": "79bca299",
      "cell_type": "code",
      "source": [
        "network = [\n",
        "    Linear(4, 8),\n",
        "    ReLU(),\n",
        "    Linear(8, 3),\n",
        "    Softmax()\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.384730Z",
          "iopub.execute_input": "2026-02-25T12:11:12.385211Z",
          "iopub.status.idle": "2026-02-25T12:11:12.459476Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.385173Z",
          "shell.execute_reply": "2026-02-25T12:11:12.458359Z"
        },
        "id": "79bca299"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f49a91f8",
      "cell_type": "markdown",
      "source": [
        "## 7) Forward / Backward génériques\n",
        "\n",
        "- `forward` applique chaque couche dans l'ordre\n",
        "- `backward` applique les couches en sens inverse"
      ],
      "metadata": {
        "id": "f49a91f8"
      }
    },
    {
      "id": "85eb89ff",
      "cell_type": "code",
      "source": [
        "def forward(network, X):\n",
        "    out = X\n",
        "    for layer in network:\n",
        "        out = layer.forward(out)\n",
        "    return out\n",
        "\n",
        "def backward(network, grad, learning_rate):\n",
        "    for layer in reversed(network):\n",
        "        grad = layer.backward(grad, learning_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.460963Z",
          "iopub.execute_input": "2026-02-25T12:11:12.461345Z",
          "iopub.status.idle": "2026-02-25T12:11:12.491106Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.461308Z",
          "shell.execute_reply": "2026-02-25T12:11:12.489924Z"
        },
        "id": "85eb89ff"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33bf3bab",
      "cell_type": "markdown",
      "source": [
        "## 8) Entraînement\n",
        "\n",
        "### Travail demandé\n",
        "Implémenter la boucle d'entraînement :\n",
        "\n",
        "1. Forward sur `X_train`\n",
        "2. Calculer la loss cross-entropy\n",
        "3. Calculer le gradient simplifié CE + Softmax\n",
        "4. Backward + mise à jour des poids\n",
        "5. Stocker loss et accuracy\n",
        "\n",
        "Hyperparamètres suggérés :\n",
        "- `epochs = 500`\n",
        "- `learning_rate = 0.5`\n",
        "\n",
        "### Rappels (pour clarifier les formes)\n",
        "\n",
        "Si le réseau produit des logits $Z \\in \\mathbb{R}^{N \\times C}$ (avant softmax) et des prédictions\n",
        "$\\hat{Y} = \\mathrm{softmax}(Z)$, alors la cross-entropy multi-classes (one-hot) est :\n",
        "\n",
        "$$\n",
        "L = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C} y_{ic}\\,\\log(\\hat{y}_{ic})\n",
        "$$\n",
        "\n",
        "Le gradient simplifié (CE + Softmax) **par rapport aux logits** $Z$ est :\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial Z} = \\frac{\\hat{Y} - Y}{N}\n",
        "$$\n",
        "\n",
        "> **Note technique** : le gradient simplifié CE+Softmax est calculé wrt les logits **avant** softmax.  \n",
        "> Comme notre réseau termine par `Softmax()`, on appliquera le backward sur `network[:-1]` (on “saute” la couche Softmax en backward)."
      ],
      "metadata": {
        "id": "33bf3bab"
      }
    },
    {
      "id": "10fac7a4",
      "cell_type": "code",
      "source": [
        "def predict_proba(network, X):\n",
        "    return forward(network, X)\n",
        "\n",
        "def predict_label(network, X):\n",
        "    proba = predict_proba(network, X)\n",
        "    return np.argmax(proba, axis=1)\n",
        "\n",
        "epochs = 500\n",
        "learning_rate = 0.5\n",
        "\n",
        "loss_history = []\n",
        "acc_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # TODO: forward\n",
        "    y_pred = predict_proba(network, X_train)\n",
        "    # TODO: loss\n",
        "    loss = cross_entropy_loss(y_pred, Y_train)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    # TODO: accuracy train\n",
        "    y_hat = predict_label(network, X_train)\n",
        "    acc = sum(y1 == y2 for y1, y2 in zip(y_hat, y_train)) / len(y_hat)\n",
        "    acc_history.append(acc)\n",
        "\n",
        "    # TODO: gradient CE+Softmax (wrt logits)\n",
        "    grad = (y_pred - Y_train)/len(y_pred)\n",
        "    # TODO: backward (sans la couche Softmax)\n",
        "    backward(network[:-1], grad, learning_rate)\n",
        "\n",
        "    if epoch % 25 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | loss={loss:.4f} | acc={acc:.3f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:12.492290Z",
          "iopub.execute_input": "2026-02-25T12:11:12.492881Z",
          "iopub.status.idle": "2026-02-25T12:11:13.069304Z",
          "shell.execute_reply.started": "2026-02-25T12:11:12.492840Z",
          "shell.execute_reply": "2026-02-25T12:11:13.068199Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10fac7a4",
        "outputId": "e74ec063-e4a3-4f56-90ad-84d6ed092368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 025 | loss=1.0897 | acc=0.667\n",
            "Epoch 050 | loss=0.6864 | acc=0.700\n",
            "Epoch 075 | loss=0.4247 | acc=0.883\n",
            "Epoch 100 | loss=0.3098 | acc=0.942\n",
            "Epoch 125 | loss=0.2310 | acc=0.950\n",
            "Epoch 150 | loss=0.1801 | acc=0.950\n",
            "Epoch 175 | loss=0.1481 | acc=0.950\n",
            "Epoch 200 | loss=0.1371 | acc=0.958\n",
            "Epoch 225 | loss=0.1368 | acc=0.958\n",
            "Epoch 250 | loss=0.1021 | acc=0.967\n",
            "Epoch 275 | loss=0.0936 | acc=0.975\n",
            "Epoch 300 | loss=0.0872 | acc=0.975\n",
            "Epoch 325 | loss=0.0821 | acc=0.975\n",
            "Epoch 350 | loss=0.0780 | acc=0.975\n",
            "Epoch 375 | loss=0.0747 | acc=0.975\n",
            "Epoch 400 | loss=0.0720 | acc=0.975\n",
            "Epoch 425 | loss=0.0697 | acc=0.975\n",
            "Epoch 450 | loss=0.0678 | acc=0.975\n",
            "Epoch 475 | loss=0.0661 | acc=0.975\n",
            "Epoch 500 | loss=0.0647 | acc=0.975\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "3b65fd95",
      "cell_type": "code",
      "source": [
        "# TODO: prédictions sur test\n",
        "y_test_pred = predict_label(network, X_test)\n",
        "test_acc = sum(y1 == y2 for y1, y2 in zip(y_test_pred, y_test)) / len(y_test_pred)\n",
        "print(\"Accuracy test:\", test_acc)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-25T12:11:13.070446Z",
          "iopub.execute_input": "2026-02-25T12:11:13.070896Z",
          "iopub.status.idle": "2026-02-25T12:11:13.076913Z",
          "shell.execute_reply.started": "2026-02-25T12:11:13.070847Z",
          "shell.execute_reply": "2026-02-25T12:11:13.075962Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b65fd95",
        "outputId": "d93049ac-47e8-42ec-e034-3b09fd68bd66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy test: 1.0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "id": "066cff2a",
      "cell_type": "markdown",
      "source": [
        "## 11) Questions (à compléter)\n",
        "\n",
        "Répondez ici :\n",
        "\n",
        "1. Quel est le rôle de ReLU ?\n",
        "2. Pourquoi utilise-t-on Softmax en sortie ?\n",
        "3. Quel est le rôle de la cross-entropy ?\n",
        "4. Que se passe-t-il si le learning rate est trop grand ? Trop petit ?\n",
        "5. Pourquoi séparer train/test ?"
      ],
      "metadata": {
        "id": "066cff2a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40906e64"
      },
      "source": [
        "## 11) Questions (à compléter)\n",
        "\n",
        "Répondez ici :\n",
        "\n",
        "1.  **Quel est le rôle de ReLU ?**\n",
        "    La fonction d'activation ReLU (Rectified Linear Unit) introduit de la non-linéarité dans le réseau de neurones. Sans elle, le réseau se comporterait comme un simple modèle linéaire, quelles que soient le nombre de couches. ReLU permet au réseau d'apprendre des relations complexes dans les données en activant seulement les neurones dont l'entrée est positive, ce qui aide également à résoudre le problème de la disparition du gradient (vanishing gradient problem) par rapport à d'autres fonctions comme la sigmoïde ou la tanh pour les valeurs positives.\n",
        "\n",
        "2.  **Pourquoi utilise-t-on Softmax en sortie ?**\n",
        "    Softmax est utilisée en sortie pour les problèmes de classification multi-classes. Elle convertit un vecteur de nombres réels (logits) en une distribution de probabilité. Cela signifie que la somme des sorties sera égale à 1, et chaque sortie peut être interprétée comme la probabilité que l'entrée appartienne à une classe spécifique. C'est particulièrement utile pour obtenir des prédictions claires et interprétables pour chaque classe.\n",
        "\n",
        "3.  **Quel est le rôle de la cross-entropy ?**\n",
        "    La cross-entropy est une fonction de perte (loss function) couramment utilisée pour les problèmes de classification. Elle mesure la différence entre la distribution de probabilité prédite par le modèle (softmax en sortie) et la distribution de probabilité réelle (labels one-hot). L'objectif de l'entraînement est de minimiser cette perte, ce qui pousse le modèle à faire des prédictions plus proches des vrais labels. Une valeur de cross-entropy faible indique que le modèle est confiant et correct dans ses prédictions.\n",
        "\n",
        "4.  **Que se passe-t-il si le learning rate est trop grand ? Trop petit ?**\n",
        "    *   **Trop grand :** Si le learning rate est trop grand, le modèle risque de"
      ],
      "id": "40906e64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55d16732"
      },
      "source": [
        "    \"sauter\" par-dessus le minimum de la fonction de perte, ce qui peut entraîner une divergence de l'entraînement ou un comportement très instable avec la perte qui oscille fortement.\n",
        "    *   **Trop petit :** Si le learning rate est trop petit, le modèle convergera très lentement. L'entraînement prendra beaucoup plus de temps pour atteindre un bon niveau de performance, et il pourrait même rester bloqué dans un minimum local sans jamais atteindre la solution optimale.\n",
        "\n",
        "5.  **Pourquoi séparer train/test ?**\n",
        "    Séparer les données en ensembles d'entraînement (train) et de test (test) est crucial pour évaluer la capacité de généralisation d'un modèle. L'ensemble d'entraînement est utilisé pour ajuster les poids du modèle. L'ensemble de test, quant à lui, est composé de données que le modèle n'a jamais vues pendant l'entraînement. En évaluant le modèle sur cet ensemble, on peut obtenir une estimation impartiale de ses performances sur de nouvelles données et détecter le surapprentissage (overfitting), où le modèle a mémorisé les données d'entraînement au lieu d'apprendre les motifs sous-jacents."
      ],
      "id": "55d16732"
    },
    {
      "id": "6a452b38",
      "cell_type": "markdown",
      "source": [
        "## Bonus (optionnel)\n",
        "\n",
        "1. Tester d'autres tailles de couche cachée : 4→4→3, 4→16→3\n",
        "2. Ajouter une deuxième couche cachée : 4→8→8→3\n",
        "3. (Optionnel) Ajouter une régularisation L2 sur les poids"
      ],
      "metadata": {
        "id": "6a452b38"
      }
    }
  ]
}